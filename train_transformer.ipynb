{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import librosa\n",
    "import warnings\n",
    "\n",
    "import mir_eval\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import join as ospj\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bs4.builder import XMLParsedAsHTMLWarning\n",
    "warnings.filterwarnings('ignore', category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    If things aren't running, try making the following command to copy the data locally:\n",
    "        rsync -a /storage/datasets/IDMT-SMT-Drums /local/<your-user>/\n",
    "'''\n",
    "\n",
    "SAMPLING_RATE = 44100\n",
    "BASEPATH = '/local/thiago.poppe/IDMT-SMT-Drums'\n",
    "\n",
    "songnames = os.listdir(ospj(BASEPATH, 'audio/'))\n",
    "songnames = list(map(lambda filename: filename.split('.')[0], songnames))\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if has_gpu else 'cpu')\n",
    "\n",
    "print('Running on device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkify_hyperparameters = {\n",
    "    'hop_length': 64,\n",
    "    'window_size': 256\n",
    "}\n",
    "\n",
    "dataloader_hyperparameters = {\n",
    "    'shuffle': True,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "optimizer_hyperparameters = {\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "training_hyperparameters = {\n",
    "    'num_epochs': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spectrogram(y):\n",
    "    spec = np.abs(librosa.feature.melspectrogram(y=y, sr=SAMPLING_RATE))\n",
    "    spec = librosa.amplitude_to_db(spec, ref=np.max)\n",
    "    \n",
    "    return spec\n",
    "\n",
    "\n",
    "def create_annotation_matrix(events, num_frames):\n",
    "    instrument2index = {'HH': 0, 'SD': 1, 'KD': 2}\n",
    "    annotations = np.zeros((3, num_frames), dtype=np.float32)\n",
    "    \n",
    "    for event in events:\n",
    "        onset = float(event.onsetsec.string)\n",
    "        instrument = event.instrument.string\n",
    "        \n",
    "        index = instrument2index[instrument]\n",
    "        onset = librosa.time_to_frames(onset, sr=SAMPLING_RATE)\n",
    "        annotations[index, onset] = 1.0\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "\n",
    "def create_feature_and_annotation(songname):\n",
    "    audiofile = ospj(BASEPATH, f'audio/{songname}.wav')\n",
    "    annotationfile = ospj(BASEPATH, f'annotation/{songname}.xml')\n",
    "    \n",
    "    with open(annotationfile, 'r') as fp:\n",
    "        soup = BeautifulSoup(fp, 'lxml')\n",
    "        events = soup.find_all('event')\n",
    "    \n",
    "    wave, sr = librosa.load(audiofile, sr=SAMPLING_RATE)\n",
    "    spec = calculate_spectrogram(wave)\n",
    "    annotation = create_annotation_matrix(events, spec.shape[1])\n",
    "    \n",
    "    return spec, annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunkifying sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating required folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting chunks folder...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(ospj(BASEPATH, 'chunks')):\n",
    "    print('Deleting chunks folder...')\n",
    "    shutil.rmtree(ospj(BASEPATH, 'chunks'))\n",
    "\n",
    "os.makedirs(ospj(BASEPATH, 'chunks'), exist_ok=True)\n",
    "os.makedirs(ospj(BASEPATH, 'chunks/train'), exist_ok=True)\n",
    "os.makedirs(ospj(BASEPATH, 'chunks/validation'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songname_type(songname):\n",
    "    pattern = re.compile(r'([a-zA-Z]+)')\n",
    "    matches = pattern.search(songname)\n",
    "    \n",
    "    # Returns songname type (RealDrum, WaveDrum or TechnoDrum)\n",
    "    return matches.group(1)\n",
    "\n",
    "songname_types = list(map(get_songname_type, songnames))\n",
    "train_songnames, validation_songnames = train_test_split(songnames, test_size=0.2, stratify=songname_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(songname, window_size=256, hop_length=64):\n",
    "    spec, annotation = create_feature_and_annotation(songname)\n",
    "    num_frames = spec.shape[1]\n",
    "    \n",
    "    for i in range(0, num_frames - window_size + 1, hop_length):\n",
    "        spec_chunk = spec[:, i:i+window_size]\n",
    "        annotation_chunk = annotation[:, i:i+window_size]\n",
    "        \n",
    "        yield spec_chunk, annotation_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:06<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 14.13it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Splitting train data...')\n",
    "for songname in tqdm(train_songnames):\n",
    "    for chunk_id, (spec, annotation) in enumerate(chunkify(songname, **chunkify_hyperparameters)):\n",
    "        filename = songname + f'_part{chunk_id:03d}'\n",
    "        filename = ospj(BASEPATH, f'chunks/train/{filename}')\n",
    "        np.savez(filename, spec=spec, annotation=annotation)\n",
    "        \n",
    "print('Splitting validation data...')\n",
    "for songname in tqdm(validation_songnames):\n",
    "    for chunk_id, (spec, annotation) in enumerate(chunkify(songname, **chunkify_hyperparameters)):\n",
    "        filename = songname + f'_part{chunk_id:03d}'\n",
    "        filename = ospj(BASEPATH, f'chunks/validation/{filename}')\n",
    "        np.savez(filename, spec=spec, annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Network (*Rhythm Transformer*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing class imbalance weights in training set\n",
    "- Class weights computed following the suggestions on the [PyTorch's BCEWithLogitsLoss webpage](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([27.0182, 77.3636, 53.8848], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "total_frames = 0\n",
    "presence_counter = np.zeros(3)\n",
    "\n",
    "for filename in glob('/local/thiago.poppe/IDMT-SMT-Drums/chunks/train/*.npz'):\n",
    "    data = np.load(filename)\n",
    "    total_frames += data['annotation'].shape[1]\n",
    "    presence_counter += (data['annotation'] == 1).sum(axis=1)\n",
    "\n",
    "pos_weight = torch.from_numpy((total_frames - presence_counter) / presence_counter).to(device)\n",
    "print('Class weights:', pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionDataset(Dataset):\n",
    "    CHUNKS_PATH = '/local/thiago.poppe/IDMT-SMT-Drums/chunks/'\n",
    "    \n",
    "    def __init__(self, is_train: bool):\n",
    "        split = 'train' if is_train else 'validation'\n",
    "        self.filenames = glob(ospj(self.CHUNKS_PATH, split, '*.npz'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.filenames[idx])\n",
    "        return data['spec'].T, data['annotation'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(TranscriptionDataset(is_train=True), **dataloader_hyperparameters)\n",
    "validation_dataloader = DataLoader(TranscriptionDataset(is_train=False), **dataloader_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256, 128]), torch.Size([32, 256, 3]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if DataLoader output has correct shape\n",
    "for X, y in train_dataloader:\n",
    "    break\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=128,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=2,\n",
    "            num_decoder_layers=2,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.5,\n",
    "            batch_first=True,\n",
    "        )    \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(), nn.Dropout(),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DrumTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "          (dropout3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "          (dropout3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DrumTransformer().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if forward is correct\n",
    "for X, y in train_dataloader:\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    break\n",
    "    \n",
    "outputs = model(X)\n",
    "assert outputs.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3864, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if loss function works\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "criterion(outputs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_dataloader, criterion):\n",
    "    val_loss = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in validation_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    val_loss = np.mean(val_loss)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model...\n",
      "[epoch 1/200] --> train loss: 1.35467, validation loss: 1.260717\n",
      "Saving new best model...\n",
      "[epoch 10/200] --> train loss: 0.74510, validation loss: 0.655588\n",
      "Saving new best model...\n",
      "[epoch 20/200] --> train loss: 0.74053, validation loss: 0.648601\n",
      "Saving new best model...\n",
      "[epoch 30/200] --> train loss: 0.73330, validation loss: 0.633292\n",
      "Saving new best model...\n",
      "[epoch 40/200] --> train loss: 0.70642, validation loss: 0.620975\n",
      "Saving new best model...\n",
      "[epoch 50/200] --> train loss: 0.69326, validation loss: 0.595422\n",
      "[epoch 60/200] --> train loss: 0.69165, validation loss: 0.612897\n",
      "[epoch 70/200] --> train loss: 0.69655, validation loss: 0.596512\n",
      "Saving new best model...\n",
      "[epoch 80/200] --> train loss: 0.66747, validation loss: 0.575930\n",
      "[epoch 90/200] --> train loss: 0.67398, validation loss: 0.597754\n",
      "[epoch 100/200] --> train loss: 0.68770, validation loss: 0.657561\n",
      "[epoch 110/200] --> train loss: 0.67500, validation loss: 0.636732\n"
     ]
    }
   ],
   "source": [
    "num_epochs = training_hyperparameters['num_epochs']\n",
    "\n",
    "model = DrumTransformer().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), **optimizer_hyperparameters)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(X)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch == 1 or epoch % 10 == 0:\n",
    "        train_loss = np.mean(epoch_loss)\n",
    "        validation_loss = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "        if validation_loss < best_validation_loss:\n",
    "            print('Saving new best model...')\n",
    "            best_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), 'drum_transformer.ckpt')\n",
    "        \n",
    "        print(f'[epoch {epoch}/{num_epochs}] --> train loss: {train_loss:.5f}, validation loss: {validation_loss:5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "best_model = DrumTransformer().to(device)\n",
    "best_model.load_state_dict(torch.load('drum_transformer.ckpt'))\n",
    "\n",
    "total_size = 0\n",
    "recall_scores = np.zeros(3)\n",
    "fmeasure_scores = np.zeros(3)\n",
    "precision_scores = np.zeros(3)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in validation_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = best_model(X)\n",
    "        y_numpy = y.detach().cpu().numpy()\n",
    "        predictions = sigmoid(outputs).detach().cpu().numpy()\n",
    "\n",
    "        total_size += outputs.shape[0]\n",
    "        for batch_idx in range(outputs.shape[0]):        \n",
    "            for instrument in range(3):\n",
    "                reference_onsets = np.where(y_numpy[batch_idx, :, instrument])[0]\n",
    "                reference_onsets = librosa.frames_to_time(reference_onsets, sr=SAMPLING_RATE)\n",
    "\n",
    "                params = {'pre_max': 5, 'post_max': 5, 'pre_avg': 5, 'post_avg': 5, 'delta': 0.25, 'wait': 5}\n",
    "                estimated_onsets = librosa.util.peak_pick(predictions[batch_idx, :, instrument], **params)\n",
    "                estimated_onsets = librosa.frames_to_time(estimated_onsets, sr=SAMPLING_RATE)\n",
    "\n",
    "                if len(reference_onsets) != 0 and len(estimated_onsets) != 0:\n",
    "                    metrics = mir_eval.onset.evaluate(reference_onsets, estimated_onsets)\n",
    "                    recall_scores[instrument] += metrics['Recall']\n",
    "                    fmeasure_scores[instrument] += metrics['F-measure']\n",
    "                    precision_scores[instrument] += metrics['Precision']\n",
    "\n",
    "recall_scores /= total_size\n",
    "fmeasure_scores /= total_size\n",
    "precision_scores /= total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, instrument in enumerate(['Hi-Hat', 'Snare Drum', 'Kick Drum']):\n",
    "    print(f'{instrument} metrics:')\n",
    "    print(f'  - Mean Recall: {recall_scores[i]:5f}')\n",
    "    print(f'  - Mean Precision: {precision_scores[i]:5f}')\n",
    "    print(f'  - Mean F-Measure: {fmeasure_scores[i]:5f}')\n",
    "    print()\n",
    "    \n",
    "print('Overall drumkit metrics:')\n",
    "print(f'  - Mean Recall: {np.mean(recall_scores):5f}')\n",
    "print(f'  - Mean Precision: {np.mean(precision_scores[i]):5f}')\n",
    "print(f'  - Mean F-Measure: {np.mean(fmeasure_scores[i]):5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmony",
   "language": "python",
   "name": "harmony"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
