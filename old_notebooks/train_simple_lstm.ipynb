{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from os.path import join as ospj\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transcription.globals import BASEPATH\n",
    "from transcription.dataloaders import TranscriptionDataset\n",
    "\n",
    "from transcription.preprocessing import chunkify\n",
    "from transcription.preprocessing import create_feature_and_annotation\n",
    "\n",
    "from bs4.builder import XMLParsedAsHTMLWarning\n",
    "warnings.filterwarnings('ignore', category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import os\n",
    "# import torch\n",
    "# import shutil\n",
    "# import librosa\n",
    "# import warnings\n",
    "\n",
    "# import mir_eval\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# import numpy as np\n",
    "# import IPython.display as ipd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from glob import glob\n",
    "# from tqdm import tqdm\n",
    "# from bs4 import BeautifulSoup\n",
    "# from os.path import join as ospj\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from bs4.builder import XMLParsedAsHTMLWarning\n",
    "# warnings.filterwarnings('ignore', category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    If things aren't running, try making the following command to copy the data locally:\n",
    "        rsync -a /storage/datasets/IDMT-SMT-Drums /local/<your-user>/\n",
    "'''\n",
    "songnames = os.listdir(ospj(BASEPATH, 'audio/'))\n",
    "songnames = list(map(lambda filename: filename.split('.')[0], songnames))\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if has_gpu else 'cpu')\n",
    "\n",
    "print('Running on device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunkifying sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating required folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting chunks folder...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(ospj(BASEPATH, 'chunks')):\n",
    "    print('Deleting chunks folder...')\n",
    "    shutil.rmtree(ospj(BASEPATH, 'chunks'))\n",
    "\n",
    "os.makedirs(ospj(BASEPATH, 'chunks'), exist_ok=True)\n",
    "os.makedirs(ospj(BASEPATH, 'chunks/train'), exist_ok=True)\n",
    "os.makedirs(ospj(BASEPATH, 'chunks/validation'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songname_type(songname):\n",
    "    pattern = re.compile(r'([a-zA-Z]+)')\n",
    "    matches = pattern.search(songname)\n",
    "    \n",
    "    # Returns songname type (RealDrum, WaveDrum or TechnoDrum)\n",
    "    return matches.group(1)\n",
    "\n",
    "songname_types = list(map(get_songname_type, songnames))\n",
    "train_songnames, validation_songnames = train_test_split(songnames, test_size=0.2, stratify=songname_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:06<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 14.39it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Splitting train data...')\n",
    "for songname in tqdm(train_songnames):\n",
    "    for chunk_id, (spec, annotation) in enumerate(chunkify(songname)):\n",
    "        filename = songname + f'_part{chunk_id:03d}'\n",
    "        filename = ospj(BASEPATH, f'chunks/train/{filename}')\n",
    "        np.savez(filename, spec=spec, annotation=annotation)\n",
    "        \n",
    "print('Splitting validation data...')\n",
    "for songname in tqdm(validation_songnames):\n",
    "    for chunk_id, (spec, annotation) in enumerate(chunkify(songname)):\n",
    "        filename = songname + f'_part{chunk_id:03d}'\n",
    "        filename = ospj(BASEPATH, f'chunks/validation/{filename}')\n",
    "        np.savez(filename, spec=spec, annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Network (*Long-Short Term Memory* - LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing class imbalance weights in training set\n",
    "- Class weights computed following the suggestions on the [PyTorch's BCEWithLogitsLoss webpage](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1289/1289 [00:00<00:00, 2838.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([27.4714, 78.2659, 55.1293], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "total_frames = 0\n",
    "presence_counter = np.zeros(3)\n",
    "\n",
    "for filename in tqdm(glob('/local/thiago.poppe/IDMT-SMT-Drums/chunks/train/*.npz')):\n",
    "    data = np.load(filename)\n",
    "    total_frames += data['annotation'].shape[1]\n",
    "    presence_counter += (data['annotation'] == 1).sum(axis=1)\n",
    "\n",
    "pos_weight = torch.from_numpy((total_frames - presence_counter) / presence_counter).to(device)\n",
    "print('Class weights:', pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(TranscriptionDataset(is_train=True), batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(TranscriptionDataset(is_train=False), batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256, 128]), torch.Size([32, 256, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if DataLoader output has correct shape\n",
    "for X, y in train_dataloader:\n",
    "    break\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, \n",
    "                           dropout=0.5, batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(), nn.Dropout(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(), nn.Dropout(),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.tensor):\n",
    "        outputs, _ = self.lstm(x)\n",
    "        return self.classifier(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleLSTM().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "921600"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if forward is correct\n",
    "for X, y in train_dataloader:\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    break\n",
    "    \n",
    "outputs = model(X)\n",
    "assert outputs.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3178, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if loss function works\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "criterion(outputs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_dataloader, criterion):\n",
    "    val_loss = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in validation_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    val_loss = np.mean(val_loss)\n",
    "    return val_loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best model...\n",
      "[epoch 1/50] --> train loss: 1.35282, validation loss: 1.332384\n",
      "Saving new best model...\n",
      "[epoch 5/50] --> train loss: 0.89896, validation loss: 0.774593\n",
      "Saving new best model...\n",
      "[epoch 10/50] --> train loss: 0.85648, validation loss: 0.741579\n",
      "[epoch 15/50] --> train loss: 0.88468, validation loss: 0.756567\n",
      "Saving new best model...\n",
      "[epoch 20/50] --> train loss: 0.87111, validation loss: 0.737557\n",
      "Saving new best model...\n",
      "[epoch 25/50] --> train loss: 0.85660, validation loss: 0.722500\n",
      "[epoch 30/50] --> train loss: 0.90553, validation loss: 0.772338\n",
      "Saving new best model...\n",
      "[epoch 35/50] --> train loss: 0.83509, validation loss: 0.713120\n",
      "[epoch 40/50] --> train loss: 0.93167, validation loss: 0.775635\n",
      "[epoch 45/50] --> train loss: 0.99257, validation loss: 0.846737\n",
      "[epoch 50/50] --> train loss: 0.97869, validation loss: 0.792049\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SimpleLSTM().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for X, y in train_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(X)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch == 1 or epoch % 5 == 0:\n",
    "        train_loss = np.mean(epoch_loss)\n",
    "        validation_loss = evaluate(model, validation_dataloader, criterion)\n",
    "        \n",
    "        if validation_loss < best_validation_loss:\n",
    "            print('Saving new best model...')\n",
    "            best_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), 'simple_lstm_model.ckpt')\n",
    "        \n",
    "        print(f'[epoch {epoch}/{num_epochs}] --> train loss: {train_loss:.5f}, validation loss: {validation_loss:5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "best_model = SimpleLSTM().to(device)\n",
    "best_model.load_state_dict(torch.load('simple_lstm_model.ckpt'))\n",
    "\n",
    "total_size = 0\n",
    "recall_scores = np.zeros(3)\n",
    "fmeasure_scores = np.zeros(3)\n",
    "precision_scores = np.zeros(3)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in validation_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = best_model(X)\n",
    "        y_numpy = y.detach().cpu().numpy()\n",
    "        predictions = sigmoid(outputs).detach().cpu().numpy()\n",
    "\n",
    "        total_size += outputs.shape[0]\n",
    "        for batch_idx in range(outputs.shape[0]):        \n",
    "            for instrument in range(3):\n",
    "                reference_onsets = np.where(y_numpy[batch_idx, :, instrument])[0]\n",
    "                reference_onsets = librosa.frames_to_time(reference_onsets, sr=SAMPLING_RATE)\n",
    "\n",
    "                params = {'pre_max': 5, 'post_max': 5, 'pre_avg': 5, 'post_avg': 5, 'delta': 0.25, 'wait': 5}\n",
    "                estimated_onsets = librosa.util.peak_pick(predictions[batch_idx, :, instrument], **params)\n",
    "                estimated_onsets = librosa.frames_to_time(estimated_onsets, sr=SAMPLING_RATE)\n",
    "\n",
    "                if len(reference_onsets) != 0 and len(estimated_onsets) != 0:\n",
    "                    metrics = mir_eval.onset.evaluate(reference_onsets, estimated_onsets)\n",
    "                    recall_scores[instrument] += metrics['Recall']\n",
    "                    fmeasure_scores[instrument] += metrics['F-measure']\n",
    "                    precision_scores[instrument] += metrics['Precision']\n",
    "\n",
    "recall_scores /= total_size\n",
    "fmeasure_scores /= total_size\n",
    "precision_scores /= total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi-Hat metrics:\n",
      "  - Mean Recall: 0.978927\n",
      "  - Mean Precision: 0.703111\n",
      "  - Mean F-Measure: 0.809992\n",
      "\n",
      "Snare Drum metrics:\n",
      "  - Mean Recall: 0.962359\n",
      "  - Mean Precision: 0.834901\n",
      "  - Mean F-Measure: 0.874343\n",
      "\n",
      "Kick Drum metrics:\n",
      "  - Mean Recall: 0.973663\n",
      "  - Mean Precision: 0.950588\n",
      "  - Mean F-Measure: 0.960358\n",
      "\n",
      "Overall drumkit metrics:\n",
      "  - Mean Recall: 0.971650\n",
      "  - Mean Precision: 0.950588\n",
      "  - Mean F-Measure: 0.960358\n"
     ]
    }
   ],
   "source": [
    "for i, instrument in enumerate(['Hi-Hat', 'Snare Drum', 'Kick Drum']):\n",
    "    print(f'{instrument} metrics:')\n",
    "    print(f'  - Mean Recall: {recall_scores[i]:5f}')\n",
    "    print(f'  - Mean Precision: {precision_scores[i]:5f}')\n",
    "    print(f'  - Mean F-Measure: {fmeasure_scores[i]:5f}')\n",
    "    print()\n",
    "    \n",
    "print('Overall drumkit metrics:')\n",
    "print(f'  - Mean Recall: {np.mean(recall_scores):5f}')\n",
    "print(f'  - Mean Precision: {np.mean(precision_scores[i]):5f}')\n",
    "print(f'  - Mean F-Measure: {np.mean(fmeasure_scores[i]):5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmony",
   "language": "python",
   "name": "harmony"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
